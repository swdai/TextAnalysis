{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps with Textual Data\n",
    "\n",
    "The purpose of this lab is introduce first steps in natural language processing. Much of this content is based on Albrecht, Ramachandran and Winkler (2020) Blueprints for Text Analytics Using Python. O'Reilly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings\n",
    "\n",
    "Our first step is always to import the libraries that we will be using and setting defaults for formatting in Matplotlib, Pandas etc. Remember, we must activate the conda environment first before we launch this notebook in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pprint as pp\n",
    "import textwrap\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "# pandas display options\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html#available-options\n",
    "pd.options.display.max_columns = 30 # default 20\n",
    "pd.options.display.max_rows = 60 # default 60\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# np.set_printoptions(edgeitems=3) # default 3\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plot_params = {'figure.figsize': (8, 4), \n",
    "               'axes.labelsize': 'large',\n",
    "               'axes.titlesize': 'large',\n",
    "               'xtick.labelsize': 'large',\n",
    "               'ytick.labelsize':'large',\n",
    "               'figure.dpi': 100}\n",
    "# adjust matplotlib defaults\n",
    "matplotlib.rcParams.update(plot_params)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.546077Z",
     "start_time": "2021-05-26T10:42:00.397654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>51</td>\n",
       "      <td>1996</td>\n",
       "      <td>PER</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Francisco Tudela Van Breughel Douglas</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "      <td>﻿At the outset, allow me,\\nSir, to convey to you and to this Assembly the greetings\\nand congratulations of the Peruvian people, as well as\\ntheir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>56</td>\n",
       "      <td>2001</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Jack Straw</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "      <td>﻿Please allow me\\nwarmly to congratulate you, Sir, on your assumption of\\nthe presidency of the fifty-sixth session of the General\\nAssembly.\\nThi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session  year country    country_name  \\\n",
       "3871       51  1996     PER            Peru   \n",
       "4697       56  2001     GBR  United Kingdom   \n",
       "\n",
       "                                    speaker                      position  \\\n",
       "3871  Francisco Tudela Van Breughel Douglas  Minister for Foreign Affairs   \n",
       "4697                             Jack Straw  Minister for Foreign Affairs   \n",
       "\n",
       "                                                                                                                                                       text  \n",
       "3871  ﻿At the outset, allow me,\\nSir, to convey to you and to this Assembly the greetings\\nand congratulations of the Peruvian people, as well as\\ntheir...  \n",
       "4697  ﻿Please allow me\\nwarmly to congratulate you, Sir, on your assumption of\\nthe presidency of the fifty-sixth session of the General\\nAssembly.\\nThi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150 ###\n",
    "file = \"un-general-debates-blueprint.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.sample(2, random_state=53) #pull two random rows in the dataset to observe, include seed so that we can replicate\n",
    "\n",
    "#note that we can use df.sample to generate a sample data set when we have something very large in memory. \n",
    "#df2=df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting an Overview of the Data with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Summary Statistics for Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.585120Z",
     "start_time": "2021-05-26T10:42:04.548589Z"
    }
   },
   "outputs": [],
   "source": [
    "#We want to get more information about the length of each speech\n",
    "df['length'] = df['text'].str.len()\n",
    "\n",
    "#Here we will create summary statistics, but transpose the presentation\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.644901Z",
     "start_time": "2021-05-26T10:42:04.587343Z"
    }
   },
   "outputs": [],
   "source": [
    "# We would like to identify the number of countries and speakers. Data from 1970-2015 UN General Debates\n",
    "df[['country', 'speaker']].describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.673446Z",
     "start_time": "2021-05-26T10:42:04.646924Z"
    }
   },
   "outputs": [],
   "source": [
    "#We have missings on position and some under speaker\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.715677Z",
     "start_time": "2021-05-26T10:42:04.679932Z"
    }
   },
   "outputs": [],
   "source": [
    "#We can replace missings with \"unknown\" this will keep the record in the analysis\n",
    "# inplace means to store in memory\n",
    "df['speaker'].fillna('unkown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.752450Z",
     "start_time": "2021-05-26T10:42:04.720245Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['speaker'].str.contains('Bush')]['speaker'].value_counts()\n",
    "# check word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Value Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:04.948204Z",
     "start_time": "2021-05-26T10:42:04.757532Z"
    }
   },
   "outputs": [],
   "source": [
    "#Produce a box plot with speech length\n",
    "df['length'].plot(kind='box', vert=False, figsize=(8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:05.148991Z",
     "start_time": "2021-05-26T10:42:04.950770Z"
    }
   },
   "outputs": [],
   "source": [
    "#Could also create a histogram with 30 intervals, or bins.\n",
    "df['length'].plot(kind='hist', bins=30, figsize=(8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:05.439106Z",
     "start_time": "2021-05-26T10:42:05.151748Z"
    }
   },
   "outputs": [],
   "source": [
    "#Seaborn plot with gaussian kernel density estimate\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(df['length'], bins=30, kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Value Distributions across Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.102415Z",
     "start_time": "2021-05-26T10:42:05.441397Z"
    }
   },
   "outputs": [],
   "source": [
    "#catplot in seaborn makes nice visualizations to compare distributions of speech length by country\n",
    "where = df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS'])\n",
    "g = sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='box')\n",
    "g.fig.set_size_inches(4, 3) ###\n",
    "g.fig.set_dpi(100) ###\n",
    "g = sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='violin')\n",
    "g.fig.set_size_inches(4, 3) ###\n",
    "g.fig.set_dpi(100) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Developments over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.271029Z",
     "start_time": "2021-05-26T10:42:06.104222Z"
    }
   },
   "outputs": [],
   "source": [
    "#We can look at the development of number of UN members and the average speech length. \n",
    "df.groupby('year').size().plot(title=\"Number of Countries\", figsize=(6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.558946Z",
     "start_time": "2021-05-26T10:42:06.274077Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('year').agg({'length': 'mean'}) \\\n",
    "  .plot(title=\"Avg. Speech Length\", ylim=(0,30000), figsize=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Text Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with Regular Expressions\n",
    "Our goal is to go from source text to prepared tokens. Tokens are the unit of analysis we are interested in. Our steps will follow accordingly 1) Case folding (make everything lowercase), 2) Tokenization, 3) Stop word removal\n",
    "\n",
    "Tokenization is the process of extracting words from a sequence of characters. We will use white space and punctuation to split into tokens. This can be achieved using regular expressions. We will use POSIX \\p{L} which selects all Unicode letters rather than re. We include digits, letters and underscores as well as hyphens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTK has very limited words for removal. Do we need to change the words we are removing? Tokenization is very critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.582112Z",
     "start_time": "2021-05-26T10:42:06.562641Z"
    }
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)\n",
    "    # this breaks digits, hyphens, speical symbols, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.603608Z",
     "start_time": "2021-05-26T10:42:06.584627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let|s|defeat|SARS-CoV-2|together|in\n"
     ]
    }
   ],
   "source": [
    "text = \"Let's defeat SARS-CoV-2 together in 2020!\"\n",
    "tokens = tokenize(text)\n",
    "print(\"|\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Stop Words\n",
    "Stop words are determiners, auxiliaary verbs, pronouns, adverbs and so on. They often don't carry much meaning but can hide interesting content because of high frequency use. We must be sure to look up stop words included in standard methods. Libraries such as NLTK have a base list, but if you are doing content analysis, this needs to be monitored. May contain negation or may need to include added words specific to your context (here: dear, regards, must, would...) NLTK only contains 179 stop words BUT it includes wouldn't but not would!\n",
    "\n",
    "Reference: https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.788245Z",
     "start_time": "2021-05-26T10:42:06.605812Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.813022Z",
     "start_time": "2021-05-26T10:42:06.792334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively to set stopwords\n",
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.835634Z",
     "start_time": "2021-05-26T10:42:06.815532Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.857424Z",
     "start_time": "2021-05-26T10:42:06.839934Z"
    }
   },
   "outputs": [],
   "source": [
    "#Again, we need to be specific because inclusion or exclusion of terms can severally affect our word counts.\n",
    "include_stopwords = {'dear', 'regards', 'must', 'would', 'also'}\n",
    "exclude_stopwords = {'against'}\n",
    "\n",
    "# Twitter abbreviations\n",
    "#use union for inclusion and difference for exclusion\n",
    "stopwords |= include_stopwords\n",
    "stopwords -= exclude_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Pipeline with one Line of Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:06.877721Z",
     "start_time": "2021-05-26T10:42:06.859933Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = [str.lower, tokenize, remove_stop]\n",
    "\n",
    "def prepare(text, pipeline):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:24.578033Z",
     "start_time": "2021-05-26T10:42:06.880896Z"
    }
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].progress_apply(prepare, pipeline=pipeline)\n",
    "# progress_apply only shows the time estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:24.743332Z",
     "start_time": "2021-05-26T10:42:24.587982Z"
    }
   },
   "outputs": [],
   "source": [
    "df['num_tokens'] = df['tokens'].progress_map(len)\n",
    "# create a new var that denotes the length of tokens within each speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words with a Counter\n",
    "The counter requires tokens in order to measure frequency. In other words, we must tokenize before we can count the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:24.785986Z",
     "start_time": "2021-05-26T10:42:24.752296Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = tokenize(\"She likes my cats and my cats like my sofa.\")\n",
    "\n",
    "counter = Counter(tokens)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:24.818884Z",
     "start_time": "2021-05-26T10:42:24.790881Z"
    }
   },
   "outputs": [],
   "source": [
    "more_tokens = tokenize(\"She likes dogs and cats.\")\n",
    "counter.update(more_tokens)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:26.700698Z",
     "start_time": "2021-05-26T10:42:24.822554Z"
    }
   },
   "outputs": [],
   "source": [
    "#create ounter and run through all data\n",
    "counter = Counter()\n",
    "\n",
    "#transform into a dataframe\n",
    "_ = df['tokens'].map(counter.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:26.741941Z",
     "start_time": "2021-05-26T10:42:26.702480Z"
    }
   },
   "outputs": [],
   "source": [
    "pp.pprint(counter.most_common(5))\n",
    "# pp.pprint gives printout in pairs\n",
    "# good indication of how to tweak the stopwords, exclude high-freq words that doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:26.765301Z",
     "start_time": "2021-05-26T10:42:26.743907Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter ###\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:28.914276Z",
     "start_time": "2021-05-26T10:42:26.767840Z"
    }
   },
   "outputs": [],
   "source": [
    "#alternative means of display\n",
    "freq_df = count_words(df)\n",
    "freq_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:39.172001Z",
     "start_time": "2021-05-26T10:42:28.917074Z"
    }
   },
   "outputs": [],
   "source": [
    "# top words with 10+ characters\n",
    "count_words(df, column='text', \n",
    "            preprocess=lambda text: re.findall(r\"\\w{10,}\", text)).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Frequency Diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:39.648788Z",
     "start_time": "2021-05-26T10:42:39.175200Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = freq_df.head(15).plot(kind='barh', width=0.95, figsize=(8,3))\n",
    "ax.invert_yaxis()\n",
    "ax.set(xlabel='Frequency', ylabel='Token', title='Top Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Clouds\n",
    "Remember that word clouds visualize frequency with different fot size, but they are not perfect. **Long words and words with capital letters will catch more attention, thus creating a potential bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:40.143556Z",
     "start_time": "2021-05-26T10:42:39.651100Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "text = df.query(\"year==2015 and country=='USA'\")['text'].values[0]\n",
    "\n",
    "plt.figure(figsize=(4, 2)) ###\n",
    "wc = WordCloud(max_words=100, stopwords=stopwords)\n",
    "wc.generate(text)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:40.173113Z",
     "start_time": "2021-05-26T10:42:40.146097Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud ###\n",
    "from collections import Counter ###\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save a figure using pandas: fig.savefig('/path/to/figure.pdf')\n",
    "\n",
    "For plots created using matplotlib, you can save figures by using plt.save_fig(`/path/to/fig.jpg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:41.423825Z",
     "start_time": "2021-05-26T10:42:40.175979Z"
    }
   },
   "outputs": [],
   "source": [
    "#note that here will will display the results of a word cloud with and without filtering the stop words\n",
    "freq_2015_df = count_words(df[df['year']==2015])\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)###\n",
    "wordcloud(freq_2015_df['freq'], max_words=100)\n",
    "plt.subplot(1,2,2)###\n",
    "wordcloud(freq_2015_df['freq'], max_words=100, stopwords=freq_df.head(50).index)\n",
    "#plt.tight_layout()###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking with TF-IDF\n",
    "When we want to look at slices of the data, rather than the entire body. This could allow us to look at relative frequency of words in given periods. TF-IDF will highlight words whose actual word frequency in a slice is higher than the total probability would suggest. This stands for term frequency with the inverse document frequency. We are measuring what stands out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:41.457725Z",
     "start_time": "2021-05-26T10:42:41.427473Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_idf(df, column='tokens', preprocess=None, min_df=2):\n",
    "\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(set(tokens))\n",
    "\n",
    "    # count tokens\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # create data frame and compute idf\n",
    "    idf_df = pd.DataFrame.from_dict(counter, orient='index', columns=['df'])\n",
    "    idf_df = idf_df.query('df >= @min_df')\n",
    "    idf_df['idf'] = np.log(len(df)/idf_df['df'])+0.1\n",
    "    idf_df.index.name = 'token'\n",
    "    return idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:44.140253Z",
     "start_time": "2021-05-26T10:42:41.460618Z"
    }
   },
   "outputs": [],
   "source": [
    "idf_df = compute_idf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:44.178293Z",
     "start_time": "2021-05-26T10:42:44.145248Z"
    }
   },
   "outputs": [],
   "source": [
    "# high IDF means rare (interesting) term\n",
    "idf_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:44.414313Z",
     "start_time": "2021-05-26T10:42:44.180732Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df['tfidf'] = freq_df['freq'] * idf_df['idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:44.464686Z",
     "start_time": "2021-05-26T10:42:44.416567Z"
    }
   },
   "outputs": [],
   "source": [
    "# for more data: joining is faster\n",
    "freq_df = freq_df.join(idf_df)\n",
    "freq_df['tfidf'] = freq_df['freq'] * freq_df['idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:48.388348Z",
     "start_time": "2021-05-26T10:42:44.468397Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we will look at plain counts vs TF-IDF for various data slices\n",
    "freq_1970 = count_words(df[df['year'] == 1970])\n",
    "freq_2015 = count_words(df[df['year'] == 2015])\n",
    "\n",
    "freq_1970['tfidf'] = freq_1970['freq'] * idf_df['idf']\n",
    "freq_2015['tfidf'] = freq_2015['freq'] * idf_df['idf']\n",
    "\n",
    "plt.figure(figsize=(12,6)) ###\n",
    "#wordcloud(freq_df['freq'], title='All years', subplot=(1,3,1))\n",
    "plt.subplot(2,2,1)###\n",
    "wordcloud(freq_1970['freq'], title='1970 - TF', \n",
    "          stopwords=['twenty-fifth', 'twenty-five'])\n",
    "plt.subplot(2,2,2)###\n",
    "wordcloud(freq_2015['freq'], title='2015 - TF', \n",
    "          stopwords=['seventieth'])\n",
    "plt.subplot(2,2,3)###\n",
    "wordcloud(freq_1970['tfidf'], title='1970 - TF-IDF', \n",
    "          stopwords=['twenty-fifth', 'twenty-five', 'twenty', 'fifth'])\n",
    "plt.subplot(2,2,4)###\n",
    "wordcloud(freq_2015['tfidf'], title='2015 - TF-IDF', \n",
    "          stopwords=['seventieth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a Keyword in Context (KWIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** textacy's API had major changes from version 0.10.1 to 0.11.  \n",
    "Here, `textacy.text_utils.KWIC` became `textacy.extract.kwic.keyword_in_context` (see [textacy documentation](https://textacy.readthedocs.io/en/latest/api_reference/extract.html#module-textacy.extract.kwic)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T05:09:08.234120Z",
     "start_time": "2021-06-04T05:09:05.181494Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "if textacy.__version__ < '0.11': # as in printed book\n",
    "    from textacy.text_utils import KWIC\n",
    "    \n",
    "else: # for textacy 0.11.x\n",
    "    from textacy.extract.kwic import keyword_in_context\n",
    "\n",
    "    def KWIC(*args, **kwargs):\n",
    "        # call keyword_in_context with all params except 'print_only'\n",
    "        return keyword_in_context(*args, \n",
    "                           **{kw: arg for kw, arg in kwargs.items() \n",
    "                            if kw != 'print_only'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:58:00.290940Z",
     "start_time": "2021-05-26T10:58:00.270371Z"
    }
   },
   "outputs": [],
   "source": [
    "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
    "\n",
    "    def add_kwic(text):\n",
    "        kwic_list.extend(KWIC(text, keyword, ignore_case=True, \n",
    "                              window_width=window, print_only=False))\n",
    "\n",
    "    kwic_list = []\n",
    "    doc_series.progress_map(add_kwic)\n",
    "\n",
    "    if print_samples is None or print_samples==0:\n",
    "        return kwic_list\n",
    "    else:\n",
    "        k = min(print_samples, len(kwic_list))\n",
    "        print(f\"{k} random samples out of {len(kwic_list)} \" + \\\n",
    "              f\"contexts for '{keyword}':\")\n",
    "        for sample in random.sample(list(kwic_list), k):\n",
    "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
    "                  sample[1]+'  '+\\\n",
    "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:58:04.386669Z",
     "start_time": "2021-05-26T10:58:04.296378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(22) ###\n",
    "kwic(df[df['year'] == 2015]['text'], 'sdgs', print_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing N-Grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.206581Z",
     "start_time": "2021-05-26T10:41:58.405Z"
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(tokens, n=2, sep=' '):\n",
    "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]\n",
    "\n",
    "text = \"the visible manifestation of the global climate change\"\n",
    "tokens = tokenize(text)\n",
    "print(\"|\".join(ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.208551Z",
     "start_time": "2021-05-26T10:41:58.407Z"
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(tokens, n=2, sep=' ', stopwords=set()):\n",
    "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])\n",
    "            if len([t for t in ngram if t in stopwords])==0]\n",
    "\n",
    "print(\"Bigrams:\", \"|\".join(ngrams(tokens, 2, stopwords=stopwords)))\n",
    "print(\"Trigrams:\", \"|\".join(ngrams(tokens, 3, stopwords=stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.210075Z",
     "start_time": "2021-05-26T10:41:58.410Z"
    }
   },
   "outputs": [],
   "source": [
    "df['bigrams'] = df['text'].progress_apply(prepare, pipeline=[str.lower, tokenize]) \\\n",
    "                          .progress_apply(ngrams, n=2, stopwords=stopwords)\n",
    "\n",
    "count_words(df, 'bigrams').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.211195Z",
     "start_time": "2021-05-26T10:41:58.413Z"
    }
   },
   "outputs": [],
   "source": [
    "idf_df = compute_idf(df) ### re-initialize to be safe\n",
    "# concatenate existing IDF data frame with bigram IDFs\n",
    "idf_df = pd.concat([idf_df, compute_idf(df, 'bigrams', min_df=10)])\n",
    "\n",
    "freq_df = count_words(df[df['year'] == 2015], 'bigrams')\n",
    "freq_df['tfidf'] = freq_df['freq'] * idf_df['idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.214118Z",
     "start_time": "2021-05-26T10:41:58.416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6)) ###\n",
    "plt.subplot(1,2,1) ###\n",
    "wordcloud(freq_df['tfidf'], title='all bigrams', max_words=50)\n",
    "\n",
    "plt.subplot(1,2,2) ###\n",
    "# plt.tight_layout() ###\n",
    "where = freq_df.index.str.contains('climate')\n",
    "wordcloud(freq_df[where]['freq'], title='\"climate\" bigrams', max_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Frequencies across Time-Intervals and Categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Frequency Timelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.216415Z",
     "start_time": "2021-05-26T10:41:58.420Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_keywords(tokens, keywords): \n",
    "    tokens = [t for t in tokens if t in keywords]\n",
    "    counter = Counter(tokens)\n",
    "    return [counter.get(k, 0) for k in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.218645Z",
     "start_time": "2021-05-26T10:41:58.423Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords = ['nuclear', 'terrorism', 'climate', 'freedom']\n",
    "tokens = ['nuclear', 'climate', 'climate', 'freedom', 'climate', 'freedom']\n",
    "\n",
    "print(count_keywords(tokens, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.221158Z",
     "start_time": "2021-05-26T10:41:58.426Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_keywords_by(df, by, keywords, column='tokens'):\n",
    "    \n",
    "    df = df.reset_index(drop=True) # if the supplied dataframe has gaps in the index\n",
    "    freq_matrix = df[column].progress_apply(count_keywords, keywords=keywords)\n",
    "    freq_df = pd.DataFrame.from_records(freq_matrix, columns=keywords)\n",
    "    freq_df[by] = df[by] # copy the grouping column(s)\n",
    "    \n",
    "    return freq_df.groupby(by=by).sum().sort_values(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.223999Z",
     "start_time": "2021-05-26T10:41:58.428Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_keywords_by(df, by='year', keywords=keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.225541Z",
     "start_time": "2021-05-26T10:41:58.430Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.226810Z",
     "start_time": "2021-05-26T10:41:58.432Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.228254Z",
     "start_time": "2021-05-26T10:41:58.434Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df.plot(kind='line', figsize=(8, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.230717Z",
     "start_time": "2021-05-26T10:41:58.437Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(23) ###\n",
    "# analyzing mentions of 'climate' before 1980\n",
    "kwic(df.query('year < 1980')['text'], 'climate', window=35, print_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Frequency Heat Maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T10:42:49.233155Z",
     "start_time": "2021-05-26T10:41:58.441Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = ['terrorism', 'terrorist', 'nuclear', 'war', 'oil',\n",
    "            'syria', 'syrian', 'refugees', 'migration', 'peacekeeping', \n",
    "            'humanitarian', 'climate', 'change', 'sustainable', 'sdgs']  \n",
    "\n",
    "freq_df = count_keywords_by(df, by='year', keywords=keywords)\n",
    "\n",
    "# compute relative frequencies based on total number of tokens per year\n",
    "freq_df = freq_df.div(df.groupby('year')['num_tokens'].sum(), axis=0)\n",
    "# apply square root as sublinear filter for better contrast\n",
    "freq_df = freq_df.apply(np.sqrt)\n",
    "\n",
    "plt.figure(figsize=(10, 3)) ###\n",
    "sns.set(font_scale=1) ###\n",
    "sns.heatmap(data=freq_df.T, \n",
    "            xticklabels=True, yticklabels=True, cbar=False, cmap=\"Reds\")\n",
    "sns.set(font_scale=1) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n",
    "\n",
    "In the spirit of good housekeeping- we must remember to carefully save and close out of our Jupyter notebook. You will want to select \"close and halt\" under File. After closing the browser window, return to your open terminal session. Enter ctrl+c and confirm that you want to end the Jupyter Notebook session. Finally, please remember to type \"conda deactivate\" at the prompt to close out of the activated conda environment and return to the base environment. \n",
    "\n",
    "Next time, when you wish to use this environment, remember that you will open the terminal and type \"conda activate pathTofile\" at the resulting environment prompt, you will type \"jupyter notebook pathTofile\" to reopen this notebook. Drag and drop to indicate pathTofile works well on mac computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.542px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
