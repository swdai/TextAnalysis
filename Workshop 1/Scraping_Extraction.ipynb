{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Websites and Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "The example provided is adapted from Albrecht, RamachandranThe Reuters website has changed significantly and is now more complicated to search. For the examples below we will use the the Internet Archive version. We could use alternative libraries with utility functions to handle the RSS obfuscation, but this would go beyond the scope of our illustration.\n",
    "\n",
    "Several layout and formatting commands, like `figsize` to control figure size or subplot commands are not necessary, but formating preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Python Settings\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pprint as pp\n",
    "import textwrap\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "# pandas display options\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html#available-options\n",
    "pd.options.display.max_columns = 30 # default 20\n",
    "pd.options.display.max_rows = 60 # default 60\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# np.set_printoptions(edgeitems=3) # default 3\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plot_params = {'figure.figsize': (8, 4), \n",
    "               'axes.labelsize': 'large',\n",
    "               'axes.titlesize': 'large',\n",
    "               'xtick.labelsize': 'large',\n",
    "               'ytick.labelsize':'large',\n",
    "               'figure.dpi': 100}\n",
    "# adjust matplotlib defaults\n",
    "matplotlib.rcParams.update(plot_params)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bg6vXOpUoGq4"
   },
   "source": [
    "# Download and interpret robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgVenIOToGq7"
   },
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(\"https://www.reuters.com/robots.txt\")\n",
    "rp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuxWAarsoGq-"
   },
   "outputs": [],
   "source": [
    "rp.can_fetch(\"*\", \"https://www.reuters.com/arc/outboundfeeds/news-sitemap/?outputType=xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3L101T-5oGrB"
   },
   "outputs": [],
   "source": [
    "rp.can_fetch(\"*\", \"https://www.reuters.com/finance/stocks/option\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzwDF3nOoGrD"
   },
   "source": [
    "# Finding URLs from sitemap.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W852mFHOoGrE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# might need to install xmltodict\n",
    "%conda install xmltodict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import requests\n",
    "\n",
    "sitemap = xmltodict.parse(requests.get('https://www.reuters.com/arc/outboundfeeds/news-sitemap/?outputType=xml').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "841uffIloGrH"
   },
   "outputs": [],
   "source": [
    "# just see some of the URLs\n",
    "urls = [url[\"loc\"] for url in sitemap[\"urlset\"][\"url\"]]\n",
    "print(\"\\n\".join(urls[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiIUx1sSoGrL"
   },
   "source": [
    "# Finding URLs from RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters removed its RSS feed. However, we can use a saved copy from the Internet archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to install feedparser\n",
    "%conda install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MGKtBAKoGrM"
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "feed = feedparser.parse('http://web.archive.org/web/20200613003232if_/http://feeds.reuters.com/Reuters/worldNews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gEmDJtRoGrP"
   },
   "outputs": [],
   "source": [
    "[(e.title, e.link) for e in feed.entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJoqad-EoGrS"
   },
   "outputs": [],
   "source": [
    "[e.id for e in feed.entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chlsxEoAoGrU"
   },
   "source": [
    "## Downloading HTML pages with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIwBH0daoGrV"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s = requests.Session()\n",
    "for url in urls[0:10]:\n",
    "    # get the part after the last / in URL and use as filename\n",
    "    file = url.split(\"/\")[-1]\n",
    "    \n",
    "    r = s.get(url)\n",
    "    with open(file, \"w+b\") as f:\n",
    "        f.write(r.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlBeX4pWoGrY"
   },
   "outputs": [],
   "source": [
    "with open(\"urls.txt\", \"w+b\") as f:\n",
    "    f.write(\"\\n\".join(urls).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71lE3mouoGra"
   },
   "source": [
    "# Extraction with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsiYQP65oGrb"
   },
   "source": [
    "We first have to download a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "js5blgHuoGrc"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.reuters.com/article/us-health-vaping-marijuana-idUSKBN1WG4KT'\n",
    "\n",
    "file = url.split(\"/\")[-1] + \".html\"\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(file, \"w+\") as f:\n",
    "    f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W9USWj5oGrg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "with open(file, \"r\") as f:\n",
    "    html = f.read()\n",
    "    g = re.search(r'<title>(.*)</title>', html, re.MULTILINE|re.DOTALL)\n",
    "    if g:\n",
    "        print(g.groups()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YoId1bpoGrj"
   },
   "source": [
    "# Using an HTML parser for extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the articles from the Internet archive which still has the old HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA_PREFIX = \"http://web.archive.org/web/20200118131624/\"\n",
    "html = s.get(WA_PREFIX + url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WD_RSPvooGrj"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup.select(\"h1.ArticleHeader_headline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dlnd63dmoGrn"
   },
   "source": [
    "## Extracting the title/headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgQCE-sxoGrn"
   },
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZCqzQMWoGrr"
   },
   "outputs": [],
   "source": [
    "soup.h1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ew_1sC08oGru"
   },
   "outputs": [],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZuzfNv4oGry"
   },
   "outputs": [],
   "source": [
    "soup.title.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxJFTkKKoGr1"
   },
   "source": [
    "## Extracting the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0mU9XmZoGr2"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"div.StandardArticleBody_body\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptO1-Fz8oGr5"
   },
   "source": [
    "## Extracting image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0FhMlqAoGr5"
   },
   "outputs": [],
   "source": [
    "soup.select(\"div.StandardArticleBody_body figure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhvgNfE4oGr-"
   },
   "outputs": [],
   "source": [
    "soup.select(\"div.StandardArticleBody_body figure img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYs5JpEIoGsB"
   },
   "outputs": [],
   "source": [
    "soup.select(\"div.StandardArticleBody_body figcaption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3FiDGEwoGsE"
   },
   "source": [
    "## Extracting the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBSlxZ_YoGsE"
   },
   "outputs": [],
   "source": [
    "soup.find(\"link\", {'rel': 'canonical'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foiJRIbqoGsI"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"link[rel=canonical]\")['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLTcC-5noGsP"
   },
   "source": [
    "## Extracting list information (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlZ8yscxoGsP"
   },
   "outputs": [],
   "source": [
    "soup.find(\"meta\", {'name': 'Author'})['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jmcyPksoGsT"
   },
   "outputs": [],
   "source": [
    "sel = \"div.BylineBar_first-container.ArticleHeader_byline-bar div.BylineBar_byline span\"\n",
    "soup.select(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e2XK_1noGsV"
   },
   "outputs": [],
   "source": [
    "[a.text for a in soup.select(sel)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIrBNBLDoGsX"
   },
   "source": [
    "## Extracting text of links (section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJn4FH22oGsX"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"div.ArticleHeader_channel a\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zDbe8eYHoGsZ"
   },
   "source": [
    "## Extracting reading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0CXIk3CoGsa"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"p.BylineBar_reading-time\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICCaNKEKoGse"
   },
   "source": [
    "## Extracting attributes (id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh16e7BDoGsf"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"div.StandardArticle_inner-container\")['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKV2JMERoGsi"
   },
   "source": [
    "## Extracting Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJdtBuLCoGsi"
   },
   "outputs": [],
   "source": [
    "soup.select_one(\"p.Attribution_content\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIPmYxxwoGsl"
   },
   "source": [
    "## Extracting Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzY1mJF2oGsl"
   },
   "outputs": [],
   "source": [
    "ptime = soup.find(\"meta\", { 'property': \"og:article:published_time\"})['content']\n",
    "print(ptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDg1g_AOoGsn"
   },
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "parser.parse(ptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hflw65HJoGsq"
   },
   "outputs": [],
   "source": [
    "parser.parse(soup.find(\"meta\", { 'property': \"og:article:modified_time\"})['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bMrACxCoGsv"
   },
   "source": [
    "## Spidering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sN3CuAbLoGsw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from dateutil import parser\n",
    "\n",
    "def download_archive_page(page):\n",
    "    filename = \"page-%06d.html\" % page\n",
    "    if not os.path.isfile(filename):\n",
    "        url = \"https://www.reuters.com/news/archive/\" + \\\n",
    "              \"?view=page&page=%d&pageSize=10\" % page\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"w+\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "def parse_archive_page(page_file):\n",
    "    with open(page_file, \"r\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    hrefs = [\"https://www.reuters.com\" + a['href'] \n",
    "               for a in soup.select(\"article.story div.story-content a\")]\n",
    "    return hrefs\n",
    "\n",
    "def download_article(url):\n",
    "    # check if article already there\n",
    "    filename = url.split(\"/\")[-1] + \".html\"\n",
    "    if not os.path.isfile(filename):\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"w+\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "def parse_article(article_file):\n",
    "    def find_obfuscated_class(soup, klass):\n",
    "        return soup.find_all(lambda tag: tag.has_attr(\"class\") and (klass in \" \".join(tag[\"class\"])))\n",
    "    \n",
    "    with open(article_file, \"r\") as f:\n",
    "        html = f.read()\n",
    "    r = {}\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    r['url'] = soup.find(\"link\", {'rel': 'canonical'})['href']\n",
    "    r['id'] = r['url'].split(\"-\")[-1]\n",
    "    r['headline'] = soup.h1.text\n",
    "    r['section'] = find_obfuscated_class(soup, \"ArticleHeader-channel\")[0].text\n",
    "    \n",
    "    r['text'] = \"\\n\".join([t.text for t in find_obfuscated_class(soup, \"Paragraph-paragraph\")])\n",
    "    r['authors'] = find_obfuscated_class(soup, \"Attribution-attribution\")[0].text\n",
    "    r['time'] = soup.find(\"meta\", { 'property': \"og:article:published_time\"})['content']\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXg3w6XpoGsz"
   },
   "outputs": [],
   "source": [
    "# download 2 pages of archive\n",
    "for p in range(1, 2):\n",
    "    download_archive_page(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HiANkSUPoGs1"
   },
   "outputs": [],
   "source": [
    "# parse archive and add to article_urls\n",
    "import glob\n",
    "\n",
    "article_urls = []\n",
    "for page_file in glob.glob(\"page-*.html\"):\n",
    "    article_urls += parse_archive_page(page_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hCFPEwQoGs3"
   },
   "outputs": [],
   "source": [
    "# download articles\n",
    "for url in article_urls:\n",
    "    download_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LB9q54DZoGs8"
   },
   "outputs": [],
   "source": [
    "# arrange in pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for article_file in glob.glob(\"*-id???????????.html\"):\n",
    "    df = df.append(parse_article(article_file), ignore_index=True)\n",
    "df['time'] = pd.to_datetime(df.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgopjGE_oGtA"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxHMZZAIoGtG"
   },
   "outputs": [],
   "source": [
    "df.sort_values(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRII_OGyoGto"
   },
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the code for `scrapy` cannot be changed easily. One more argument for using *up to date* separate libraries. In this version, it still collects the titles of the articles but nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to install scrapy\n",
    "%conda install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ca8x2bg_oGto"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging\n",
    "\n",
    "\n",
    "class ReutersArchiveSpider(scrapy.Spider):\n",
    "    name = 'reuters-archive'\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'FEED_FORMAT': 'json',\n",
    "        'FEED_URI': 'reuters-archive.json'\n",
    "    }\n",
    "    \n",
    "    start_urls = [\n",
    "        'https://www.reuters.com/news/archive/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for article in response.css(\"article.story div.story-content a\"):\n",
    "            yield response.follow(article.css(\"a::attr(href)\").extract_first(), self.parse_article)\n",
    "\n",
    "        next_page_url = response.css('a.control-nav-next::attr(href)').extract_first()\n",
    "        if (next_page_url is not None) & ('page=2' not in next_page_url):\n",
    "            yield response.follow(next_page_url, self.parse)\n",
    "\n",
    "    def parse_article(self, response):\n",
    "        yield {\n",
    "          'title': response.css('h1::text').extract_first().strip(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mxs6xAtnoGtr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this can be run only once from a Jupyter notebook due to Twisted\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "\n",
    "process.crawl(ReutersArchiveSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_jfvbozoGty"
   },
   "outputs": [],
   "source": [
    "glob.glob(\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SZPLsUqoGt0"
   },
   "outputs": [],
   "source": [
    "!cat 'reuters-archive.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IrOHX-rr_fZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Crawling_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
